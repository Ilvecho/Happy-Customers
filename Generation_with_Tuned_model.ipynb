{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMLauMVUOPD+k1Ikp59GNjX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ilvecho/Happy-Customers/blob/main/Generation_with_Tuned_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook allows you to run the LoRA fine tuned model by Syllog directly on Google Colab.\n",
        "\n",
        "The model was tuned on topics relevant to HR professionals.\n",
        "\n",
        "Please use the **T4 GPU** runtime accelerator"
      ],
      "metadata": {
        "id": "6e93X-NMzzeR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Import dependencies and get GPU\n",
        "%%capture\n",
        "\n",
        "!pip install trl transformers datasets torch peft\n",
        "!pip install -qU accelerate\n",
        "!pip install -qU bitsandbytes\n",
        "!pip install thefuzz\n",
        "\n",
        "import numpy as np\n",
        "import re\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n",
        "from peft import AutoPeftModelForCausalLM, PeftConfig, PeftModel\n",
        "from thefuzz import fuzz\n",
        "\n",
        "# Might be removed in future\n",
        "from google.colab import files,drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "00LU2e610Tzj",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load the configuration"
      ],
      "metadata": {
        "id": "wtWOQWp32czy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are going to load:\n",
        "- The Bits and Bytes configuration for the quantization (needed because of resource availability)\n",
        "- The tuned model\n",
        "- The associated tokenizer\n",
        "- The pipeline used for the output generation"
      ],
      "metadata": {
        "id": "FQOv2Mv26iy5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Load the configuration\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_compute_dtype= torch.bfloat16,\n",
        "        bnb_4bit_use_double_quant= False,\n",
        ")\n",
        "\n",
        "# For the time being we load the model from Drive.\n",
        "# In the future, once we have a Syllog HuggingFace account, we will load the model from there\n",
        "PEFT_MODEL = '/content/gdrive/MyDrive/Syllog/full_results/tuned_model'\n",
        "\n",
        "# Perf configuration\n",
        "config = PeftConfig.from_pretrained(PEFT_MODEL)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    config.base_model_name_or_path,\n",
        "    return_dict=True,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "\n",
        "# Tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Load the Lora model\n",
        "model = PeftModel.from_pretrained(model, PEFT_MODEL)\n",
        "\n",
        "# Pipeline\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer = tokenizer,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\"\n",
        ")"
      ],
      "metadata": {
        "id": "Y3YfIBZV1Ni8",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that everything is loaded, you only need to ask your prompt and wait for the model to answer!"
      ],
      "metadata": {
        "id": "kv9AISrq8V9g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Input the prompt\n",
        "user_prompt = input(\"Ask me anything related to HR: \")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "ZdrJgAJ27qhW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Here is were the magic happens\n",
        "%%capture\n",
        "##############################################\n",
        "#############     GENERATION     #############\n",
        "##############################################\n",
        "\n",
        "system_message = \"Sei un assistente AI utile e conciso. Rispondi in massimo cinque frasi, va bene anche usarne meno.\"\n",
        "\n",
        "prompt_template=f\"\"\"<|im_start|>Sistema: {system_message}<|im_end|>\n",
        "<|im_start|>Utente: {user_prompt}<|im_end|>\n",
        "<|im_start|>Assistente: \"\"\"\n",
        "\n",
        "# Call the pipeline also with args to be passed to the model\n",
        "sequences = pipe(\n",
        "    prompt_template,\n",
        "    max_new_tokens=200,\n",
        "    do_sample=False,\n",
        "    return_full_text=False,\n",
        "    num_return_sequences=1,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        "    pad_token_id=tokenizer.eos_token_id,\n",
        "    decoder_start_token_id=0,\n",
        ")\n",
        "\n",
        "answer = sequences[0]['generated_text']\n",
        "\n",
        "##############################################\n",
        "#############     PROCESSING     #############\n",
        "##############################################\n",
        "\n",
        "# If there is the end tag, let's just consider what's before it\n",
        "if '<|im_end|>' in answer:\n",
        "  answer = answer.split('<|im_end|>')[0]\n",
        "\n",
        "# Then, we want to remove the numbers of the numbered item list\n",
        "answer = re.sub(r'\\d+\\.\\s*', '- ', answer)\n",
        "\n",
        "# Then, what we want  to do is to verify that each sentence generated by the model is not similar to the others\n",
        "# We want to discard the last element as the model will always close a sentence with a dot.\n",
        "# If no dot is present, it means that the generation was interrupted because of the max tokens limit\n",
        "sentences = re.split(r'[.?!:;]', answer.strip())\n",
        "\n",
        "if len(sentences[-1]) > 0:\n",
        "  answer = answer[:-len(sentences[-1])]\n",
        "\n",
        "# If there are multiple sentences, check that they are different from each other\n",
        "if len(sentences) > 1:\n",
        "  sentences = sentences[:-1]\n",
        "\n",
        "  # Build the Fuzzy matching matrix\n",
        "  size = len(sentences)\n",
        "  fuzz_match = np.zeros((size, size))\n",
        "\n",
        "  for i, sentence in enumerate(sentences):\n",
        "    for j, compare in enumerate(sentences):\n",
        "      if sentence is compare:\n",
        "        continue\n",
        "      else:\n",
        "        score = fuzz.token_set_ratio(sentence,compare)\n",
        "        fuzz_match[i][j] = score\n",
        "\n",
        "  # Discard sentences with high score\n",
        "  max_score = np.max(fuzz_match)\n",
        "  argmax_score = np.argmax(fuzz_match)\n",
        "\n",
        "  while max_score > 80:\n",
        "    # Find the two matching sentences\n",
        "    i = argmax_score // size\n",
        "    j = argmax_score % size\n",
        "\n",
        "    # print(f'Size: {size}, argmax: {argmax_score}, i: {i}, j: {j}')\n",
        "\n",
        "    # out of the two, find the one with the highest average score (the sentence on average more similar to all the others)\n",
        "    if fuzz_match[i].mean() < fuzz_match[j].mean():\n",
        "      to_delete = j\n",
        "    else:\n",
        "      assert fuzz_match[i].mean() >= fuzz_match[j].mean()\n",
        "      to_delete = i\n",
        "\n",
        "    # Delete sentence from the fuzz match\n",
        "    fuzz_match = np.delete(fuzz_match, to_delete, axis=0)\n",
        "    fuzz_match = np.delete(fuzz_match, to_delete, axis=1)\n",
        "\n",
        "    # Since we are deleting one sentence, we need to reduce the size as well\n",
        "    size -= 1\n",
        "\n",
        "    # Delete sentence from sentences\n",
        "    sentences.pop(to_delete)\n",
        "\n",
        "\n",
        "    # Values for the new While cycle\n",
        "    max_score = np.max(fuzz_match)\n",
        "    argmax_score = np.argmax(fuzz_match)\n",
        "\n",
        "  output = ''\n",
        "\n",
        "  for sentence in sentences:\n",
        "    idx = answer.find(sentence)\n",
        "\n",
        "    if idx != -1 and idx + len(sentence) < len(answer):\n",
        "        punctuation = answer[idx + len(sentence)]\n",
        "        output += sentence.strip() + punctuation + '\\n'\n",
        "    else:\n",
        "        print(\"Substring not found or character after the substring does not exist.\")\n",
        "\n",
        "else:\n",
        "  assert len(sentences) == 1\n",
        "  output = sentences[0]"
      ],
      "metadata": {
        "id": "pE2wwz2R09FK",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(output)"
      ],
      "metadata": {
        "id": "oaMw2sHN8vcY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}